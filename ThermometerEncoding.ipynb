{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thermometer encoding\n",
    "## Some util functions for convert a normal image to a one-hot way coded image\n",
    "\n",
    "### discription of a one-hot coded image:\n",
    "* n:batch size\n",
    "* w:width of a image\n",
    "* h:height of a image\n",
    "* k:k-level discretization of a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\"\"\"\n",
    "input:natural image arr:n*w*h*c\n",
    "return: quantisized image n*w*h*c\n",
    "\"\"\"\n",
    "def quantization(arr,k):\n",
    "    quant = np.zeros(arr.shape)\n",
    "    for i in range(1,k):\n",
    "        quant[arr>1.0*i/k]+=1\n",
    "    return quant\n",
    "\n",
    "\"\"\"\n",
    "input:quantisized img shape:n*w*h*c\n",
    "retun:one-hot coded image shape:n*w*h*c*k\n",
    "\"\"\"\n",
    "def onehot(arr,k):\n",
    "    n,w,h = arr.shape\n",
    "    arr = arr.reshape(n,-1)\n",
    "    enc=OneHotEncoder(n_values=k,sparse=False)\n",
    "    arr = enc.fit_transform(arr)\n",
    "    arr = arr.reshape(n,w,h,k)\n",
    "    arr = arr.transpose(0,3,1,2)\n",
    "    return arr\n",
    "\n",
    "\"\"\"\n",
    "input:one-hot coded img shape:n*w*h*c*k\n",
    "retun:trmp coded image shape:n*w*h*c*k\n",
    "\"\"\"\n",
    "def tempcode(arr,k):\n",
    "    tempcode = np.zeros(arr.shape)\n",
    "    for i in range(k):\n",
    "        tempcode[:,i,:,:] = np.sum(arr[:,:i+1,:,:],axis=1)\n",
    "    return tempcode\n",
    "    \n",
    "\"\"\"\n",
    "from a thermometerencoding image to a mormally coded image, for some visulization usage\n",
    "\"\"\"\n",
    "def temp2img(tempimg,k):\n",
    "    img = np.sum(tempimg,axis=1)\n",
    "    img = np.ones(img.shape)*(k+1)-img\n",
    "    img = img*1.0/k\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.79451513  0.08269887]\n",
      "  [ 0.31621181  0.62802222]]\n",
      "\n",
      " [[ 0.33544049  0.68535259]\n",
      "  [ 0.97273507  0.12748406]]]\n",
      "(2, 2, 2)\n",
      "[[[ 3.  0.]\n",
      "  [ 1.  2.]]\n",
      "\n",
      " [[ 1.  2.]\n",
      "  [ 3.  0.]]]\n",
      "[[[ 0.  1.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 1.  0.]]\n",
      "\n",
      " [[ 0.  0.]\n",
      "  [ 0.  1.]]\n",
      "\n",
      " [[ 1.  0.]\n",
      "  [ 0.  0.]]]\n",
      "[[[ 0.  1.]\n",
      "  [ 0.  0.]]\n",
      "\n",
      " [[ 0.  1.]\n",
      "  [ 1.  0.]]\n",
      "\n",
      " [[ 0.  1.]\n",
      "  [ 1.  1.]]\n",
      "\n",
      " [[ 1.  1.]\n",
      "  [ 1.  1.]]]\n",
      "[[[ 1.    0.25]\n",
      "  [ 0.5   0.75]]\n",
      "\n",
      " [[ 0.5   0.75]\n",
      "  [ 1.    0.25]]]\n"
     ]
    }
   ],
   "source": [
    "img = np.random.random((2, 2, 2))\n",
    "print img\n",
    "print img.shape\n",
    "quant = quantization(img,4)\n",
    "print quant\n",
    "onehotimg=onehot(quant,4)\n",
    "print onehotimg[0]\n",
    "tempcod = tempcode(onehotimg.copy(),4)\n",
    "print tempcod[0]\n",
    "recoverimg = temp2img(tempcod,4)\n",
    "print recoverimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function with attacks\n",
    "### getmask\n",
    "input image x, random perbutation $\\epsilon$, get a mask for {$x-\\epsilon$,$x+\\epsilon$}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMask(x,epsilon,k):\n",
    "    n,w,h = x.shape\n",
    "    mask = np.zeros((n,k,w,h))\n",
    "    low = x - epsilon\n",
    "    low[low < 0] = 0\n",
    "    high = x + epsilon\n",
    "    high[high > 1] = 1\n",
    "    for i in range(k+1):\n",
    "        interimg = (i*1./k)*low + (1-i*1./k)*high\n",
    "        mask+=onehot(quantization(interimg,k),k)\n",
    "    mask[mask>1] = 1\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.79451513  0.08269887]\n",
      "  [ 0.31621181  0.62802222]]\n",
      "\n",
      " [[ 0.33544049  0.68535259]\n",
      "  [ 0.97273507  0.12748406]]]\n",
      "[[[[ 0.  1.]\n",
      "   [ 0.  0.]]\n",
      "\n",
      "  [[ 0.  0.]\n",
      "   [ 1.  0.]]\n",
      "\n",
      "  [[ 0.  0.]\n",
      "   [ 0.  1.]]\n",
      "\n",
      "  [[ 1.  0.]\n",
      "   [ 0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.]\n",
      "   [ 0.  1.]]\n",
      "\n",
      "  [[ 1.  0.]\n",
      "   [ 0.  0.]]\n",
      "\n",
      "  [[ 0.  1.]\n",
      "   [ 0.  0.]]\n",
      "\n",
      "  [[ 0.  0.]\n",
      "   [ 1.  0.]]]]\n",
      "[[[[ 0.  1.]\n",
      "   [ 1.  0.]]\n",
      "\n",
      "  [[ 1.  0.]\n",
      "   [ 1.  0.]]\n",
      "\n",
      "  [[ 1.  0.]\n",
      "   [ 1.  1.]]\n",
      "\n",
      "  [[ 1.  0.]\n",
      "   [ 1.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 1.  0.]\n",
      "   [ 0.  1.]]\n",
      "\n",
      "  [[ 1.  1.]\n",
      "   [ 0.  1.]]\n",
      "\n",
      "  [[ 1.  1.]\n",
      "   [ 1.  1.]]\n",
      "\n",
      "  [[ 0.  1.]\n",
      "   [ 1.  0.]]]]\n"
     ]
    }
   ],
   "source": [
    "print img\n",
    "print onehot(quantization(img,4),4)\n",
    "print getMask(img,np.random.random(img.shape)*0.5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                   ])),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(15, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(4*4*64, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 4*4*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LS-PGD attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSPGDonechannel(data,target,epsilon,k,delta,xi,step,criterion):\n",
    "    datanumpy = data.numpy()\n",
    "    data0 = datanumpy[:,0,:,:]\n",
    "    mask = getMask(data0,epsilon,k)\n",
    "    u = np.random.random(mask.shape)-(1-mask)*1e10\n",
    "    T = 1.0\n",
    "    u = Variable(torch.Tensor(u).cuda(),requires_grad=True)\n",
    "    z = F.softmax(u/T,dim=1)\n",
    "    z = torch.cumsum(z,dim=1)\n",
    "    for t in range(step):\n",
    "        out = model(z)\n",
    "        loss = criterion(out,target)\n",
    "        if u.grad!=None:\n",
    "            u.grad.data._zero()\n",
    "        loss.backward()\n",
    "        grad = u.grad\n",
    "        u = xi*torch.sign(grad) + u\n",
    "        u = Variable(u.data,requires_grad=True)\n",
    "        z = F.softmax(u/T,dim=1)\n",
    "        z = torch.cumsum(z,dim=1)\n",
    "        T = T*delta\n",
    "    attackimg = np.argmax(u.data.cpu().numpy(),axis=1)\n",
    "    themattackimg = tempcode(onehot(attackimg,k),k)\n",
    "    return themattackimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "level=15\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "def trainnat(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = Variable(target)\n",
    "        data = data.numpy()[:,0,:,:]\n",
    "        data = Variable(torch.Tensor(tempcode(onehot(quantization(data,level),level),level)))\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target = Variable(target)\n",
    "        data = data.numpy()[:,0,:,:]\n",
    "        data = Variable(torch.Tensor(tempcode(onehot(quantization(data,level),level),level)))\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "def trainadv(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = Variable(target.cuda())\n",
    "        data = LSPGDonechannel(data=data,target=target,epsilon=0.3,k=level,delta=1.2,xi=1.0,step=2,criterion=criterion)\n",
    "        data = Variable(torch.Tensor(data).cuda())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "    \n",
    "def testadv():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target = target.cuda()\n",
    "        target = Variable(target)\n",
    "        data = LSPGDonechannel(data=data,target=target,epsilon=0.3,k=level,delta=1.2,xi=1.0,step=7,criterion=criterion)\n",
    "        data = Variable(torch.Tensor(data).cuda())\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train for 1 epoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.296473\n",
      "Train Epoch: 0 [1000/60000 (2%)]\tLoss: 2.281995\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 2.249601\n",
      "Train Epoch: 0 [3000/60000 (5%)]\tLoss: 2.161287\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 2.055316\n",
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 1.854571\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 1.619478\n",
      "Train Epoch: 0 [7000/60000 (12%)]\tLoss: 1.428817\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 1.021998\n",
      "Train Epoch: 0 [9000/60000 (15%)]\tLoss: 0.984555\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.621356\n",
      "Train Epoch: 0 [11000/60000 (18%)]\tLoss: 0.534129\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.520464\n",
      "Train Epoch: 0 [13000/60000 (22%)]\tLoss: 0.535510\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.535267\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 0.529476\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.418255\n",
      "Train Epoch: 0 [17000/60000 (28%)]\tLoss: 0.605231\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.403791\n",
      "Train Epoch: 0 [19000/60000 (32%)]\tLoss: 0.362958\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.425205\n",
      "Train Epoch: 0 [21000/60000 (35%)]\tLoss: 0.441716\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.314216\n",
      "Train Epoch: 0 [23000/60000 (38%)]\tLoss: 0.407162\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.417334\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.464954\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.395768\n",
      "Train Epoch: 0 [27000/60000 (45%)]\tLoss: 0.471428\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.384233\n",
      "Train Epoch: 0 [29000/60000 (48%)]\tLoss: 0.237195\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.230448\n",
      "Train Epoch: 0 [31000/60000 (52%)]\tLoss: 0.482738\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.280180\n",
      "Train Epoch: 0 [33000/60000 (55%)]\tLoss: 0.340350\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.230954\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.359263\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.282059\n",
      "Train Epoch: 0 [37000/60000 (62%)]\tLoss: 0.229961\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.195395\n",
      "Train Epoch: 0 [39000/60000 (65%)]\tLoss: 0.273387\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.280653\n",
      "Train Epoch: 0 [41000/60000 (68%)]\tLoss: 0.202795\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.323157\n",
      "Train Epoch: 0 [43000/60000 (72%)]\tLoss: 0.339500\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.319159\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.285635\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.215550\n",
      "Train Epoch: 0 [47000/60000 (78%)]\tLoss: 0.241237\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.217412\n",
      "Train Epoch: 0 [49000/60000 (82%)]\tLoss: 0.194460\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.184959\n",
      "Train Epoch: 0 [51000/60000 (85%)]\tLoss: 0.240396\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.319755\n",
      "Train Epoch: 0 [53000/60000 (88%)]\tLoss: 0.239770\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.099248\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.292660\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.163331\n",
      "Train Epoch: 0 [57000/60000 (95%)]\tLoss: 0.180359\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.293773\n",
      "Train Epoch: 0 [59000/60000 (98%)]\tLoss: 0.280835\n"
     ]
    }
   ],
   "source": [
    "trainnat(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test on natral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 9603/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test on LS-PGD attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0783, Accuracy: 26/10000 (0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testadv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advtrain on LS-PGD attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 4.778333\n",
      "Train Epoch: 0 [1000/60000 (2%)]\tLoss: 2.234309\n",
      "Train Epoch: 0 [2000/60000 (3%)]\tLoss: 1.926022\n",
      "Train Epoch: 0 [3000/60000 (5%)]\tLoss: 1.505311\n",
      "Train Epoch: 0 [4000/60000 (7%)]\tLoss: 1.499077\n",
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 1.308034\n",
      "Train Epoch: 0 [6000/60000 (10%)]\tLoss: 1.426866\n",
      "Train Epoch: 0 [7000/60000 (12%)]\tLoss: 1.118608\n",
      "Train Epoch: 0 [8000/60000 (13%)]\tLoss: 1.097754\n",
      "Train Epoch: 0 [9000/60000 (15%)]\tLoss: 1.017607\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 1.128312\n",
      "Train Epoch: 0 [11000/60000 (18%)]\tLoss: 0.909971\n",
      "Train Epoch: 0 [12000/60000 (20%)]\tLoss: 0.858213\n",
      "Train Epoch: 0 [13000/60000 (22%)]\tLoss: 0.842452\n",
      "Train Epoch: 0 [14000/60000 (23%)]\tLoss: 0.828172\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 1.056247\n",
      "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.659280\n",
      "Train Epoch: 0 [17000/60000 (28%)]\tLoss: 0.791528\n",
      "Train Epoch: 0 [18000/60000 (30%)]\tLoss: 0.825837\n",
      "Train Epoch: 0 [19000/60000 (32%)]\tLoss: 0.876002\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.781835\n",
      "Train Epoch: 0 [21000/60000 (35%)]\tLoss: 0.713966\n",
      "Train Epoch: 0 [22000/60000 (37%)]\tLoss: 0.578799\n",
      "Train Epoch: 0 [23000/60000 (38%)]\tLoss: 0.698011\n",
      "Train Epoch: 0 [24000/60000 (40%)]\tLoss: 0.876671\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.567656\n",
      "Train Epoch: 0 [26000/60000 (43%)]\tLoss: 0.512513\n",
      "Train Epoch: 0 [27000/60000 (45%)]\tLoss: 0.701109\n",
      "Train Epoch: 0 [28000/60000 (47%)]\tLoss: 0.562018\n",
      "Train Epoch: 0 [29000/60000 (48%)]\tLoss: 0.596192\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.703907\n",
      "Train Epoch: 0 [31000/60000 (52%)]\tLoss: 0.649177\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.605773\n",
      "Train Epoch: 0 [33000/60000 (55%)]\tLoss: 0.493994\n",
      "Train Epoch: 0 [34000/60000 (57%)]\tLoss: 0.691156\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.777433\n",
      "Train Epoch: 0 [36000/60000 (60%)]\tLoss: 0.816369\n",
      "Train Epoch: 0 [37000/60000 (62%)]\tLoss: 0.566725\n",
      "Train Epoch: 0 [38000/60000 (63%)]\tLoss: 0.738298\n",
      "Train Epoch: 0 [39000/60000 (65%)]\tLoss: 0.537736\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.648210\n",
      "Train Epoch: 0 [41000/60000 (68%)]\tLoss: 0.642458\n",
      "Train Epoch: 0 [42000/60000 (70%)]\tLoss: 0.632553\n",
      "Train Epoch: 0 [43000/60000 (72%)]\tLoss: 0.582097\n",
      "Train Epoch: 0 [44000/60000 (73%)]\tLoss: 0.750218\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.437618\n",
      "Train Epoch: 0 [46000/60000 (77%)]\tLoss: 0.482472\n",
      "Train Epoch: 0 [47000/60000 (78%)]\tLoss: 0.524253\n",
      "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.463634\n",
      "Train Epoch: 0 [49000/60000 (82%)]\tLoss: 0.587060\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.584792\n",
      "Train Epoch: 0 [51000/60000 (85%)]\tLoss: 0.620650\n",
      "Train Epoch: 0 [52000/60000 (87%)]\tLoss: 0.370396\n",
      "Train Epoch: 0 [53000/60000 (88%)]\tLoss: 0.503538\n",
      "Train Epoch: 0 [54000/60000 (90%)]\tLoss: 0.430511\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.426360\n",
      "Train Epoch: 0 [56000/60000 (93%)]\tLoss: 0.434016\n",
      "Train Epoch: 0 [57000/60000 (95%)]\tLoss: 0.404179\n",
      "Train Epoch: 0 [58000/60000 (97%)]\tLoss: 0.497083\n",
      "Train Epoch: 0 [59000/60000 (98%)]\tLoss: 0.422729\n"
     ]
    }
   ],
   "source": [
    "trainadv(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test after advtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0069, Accuracy: 7547/10000 (75%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testadv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
